{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e6dec9",
   "metadata": {},
   "source": [
    "# Test CORAL Loss Implementation\n",
    "\n",
    "This notebook tests the CORAL (Cumulative Ordinal Regression with Logistic) loss implementation on CPU with a small subset of data for quick validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc74c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NamespacePath(['c:\\\\Users\\\\z5489720\\\\OneDrive - UNSW\\\\TERM 1\\\\Github\\\\aglone\\\\autogluon\\\\autogluon', 'autogluon', '__editable__.autogluon-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_common-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_core-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_eda-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_features-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_multimodal-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_tabular-1.4.1b20251114.finder.__path_hook__', 'autogluon', '__editable__.autogluon_timeseries-1.4.1b20251114.finder.__path_hook__'])\n",
      "C:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\autogluon\\multimodal\\src\\autogluon\\multimodal\\__init__.py\n",
      "C:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\autogluon\\tabular\\src\\autogluon\\tabular\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import autogluon\n",
    "import autogluon.multimodal\n",
    "import autogluon.tabular\n",
    "\n",
    "print(autogluon.__path__)                   # namespace search paths\n",
    "print(autogluon.multimodal.__file__)        # actual file path\n",
    "print(autogluon.tabular.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d90153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20251116_014835\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.1b20251114\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Pytorch Version:    2.7.1+cpu\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Memory:         \n",
      "Total GPU Memory:   Free: 0.00 GB, Allocated: 0.00 GB, Total: 0.00 GB\n",
      "GPU Count:          0\n",
      "Memory Avail:       7.15 GB / 31.66 GB (22.6%)\n",
      "Disk Space Avail:   68.40 GB / 950.87 GB (7.2%)\n",
      "===================================================\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.1b20251114\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Pytorch Version:    2.7.1+cpu\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Memory:         \n",
      "Total GPU Memory:   Free: 0.00 GB, Allocated: 0.00 GB, Total: 0.00 GB\n",
      "GPU Count:          0\n",
      "Memory Avail:       7.15 GB / 31.66 GB (22.6%)\n",
      "Disk Space Avail:   68.40 GB / 950.87 GB (7.2%)\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Using 50 training samples for quick CPU test\n",
      "Missing files - Train: 0, Test: 0\n",
      "\n",
      "Label distribution in training data:\n",
      "rating\n",
      "1     4\n",
      "2     5\n",
      "3    16\n",
      "4    17\n",
      "5     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
      "\n",
      "============================================================\n",
      "TESTING CORAL LOSS ON CPU\n",
      "============================================================\n",
      "Loss function: coral\n",
      "Epochs: 3\n",
      "Batch size: 4\n",
      "Training samples: 50\n",
      "Test samples: 20\n",
      "============================================================\n",
      "\n",
      "Starting training with CORAL loss...\n",
      "‚ö†Ô∏è LOOK FOR DEBUG MESSAGES ABOUT LOSS FUNCTION SELECTION ‚ö†Ô∏è\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "AutoMM starts to create your model. ‚ú®‚ú®‚ú®\n",
      "\n",
      "To track the learning progress, you can open a terminal and launch Tensorboard:\n",
      "    ```shell\n",
      "    # Assume you have installed tensorboard\n",
      "    tensorboard --logdir c:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\autogluon\\AutogluonModels\\ag-20251116_014835\n",
      "    ```\n",
      "\n",
      "Seed set to 0\n",
      "Seed set to 0\n",
      "The hyperparameter name optim.learning_rate is depreciated. We recommend using the new name optim.lr instead.The deprecated hyperparameter will raise an exception starting in AutoGluon 1.4.0\n",
      "The hyperparameter name optim.learning_rate is depreciated. We recommend using the new name optim.lr instead.The deprecated hyperparameter will raise an exception starting in AutoGluon 1.4.0\n",
      "GPU Count: 0\n",
      "GPU Count to be Used: 0\n",
      "\n",
      "GPU Count: 0\n",
      "GPU Count to be Used: 0\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name              | Type                | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 12.5 M | train\n",
      "1 | validation_metric | MulticlassAccuracy  | 0      | train\n",
      "2 | loss_func         | CrossEntropyLoss    | 0      | train\n",
      "------------------------------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "49.976    Total estimated model params size (MB)\n",
      "178       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "  | Name              | Type                | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 12.5 M | train\n",
      "1 | validation_metric | MulticlassAccuracy  | 0      | train\n",
      "2 | loss_func         | CrossEntropyLoss    | 0      | train\n",
      "------------------------------------------------------------------\n",
      "12.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 M    Total params\n",
      "49.976    Total estimated model params size (MB)\n",
      "178       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926524cc62434b1d870dfb781f9947e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2540ce1fda44dcb36319a924bd34f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc47000e93b7481098a2a496989c09a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6600f792f9a94bf0b542ee45f6f1727d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val_accuracy' reached 0.30000 (best 0.30000), saving model to 'C:\\\\Users\\\\z5489720\\\\OneDrive - UNSW\\\\TERM 1\\\\Github\\\\aglone\\\\autogluon\\\\AutogluonModels\\\\ag-20251116_014835\\\\epoch=0-step=1.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d28fb782e94c1c9e8496730bd147a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f601b8cdadf1434b9bd1f2e0d73028d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2: 'val_accuracy' reached 0.30000 (best 0.30000), saving model to 'C:\\\\Users\\\\z5489720\\\\OneDrive - UNSW\\\\TERM 1\\\\Github\\\\aglone\\\\autogluon\\\\AutogluonModels\\\\ag-20251116_014835\\\\epoch=1-step=2.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646e43bc08bb4cb390f2397e65d010ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f3c05e91b34c50af0ec762a4377185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3: 'val_accuracy' reached 0.20000 (best 0.30000), saving model to 'C:\\\\Users\\\\z5489720\\\\OneDrive - UNSW\\\\TERM 1\\\\Github\\\\aglone\\\\autogluon\\\\AutogluonModels\\\\ag-20251116_014835\\\\epoch=2-step=3.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Start to fuse 3 checkpoints via the greedy soup algorithm.\n",
      "Start to fuse 3 checkpoints via the greedy soup algorithm.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4935dd315400bbcb2b839db85a668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c30178fbb843b8a819fe36e02df6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8fb824afed40b8a8817e853d68136c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoMM has created your model. üéâüéâüéâ\n",
      "\n",
      "To load the model, use the code below:\n",
      "    ```python\n",
      "    from autogluon.multimodal import MultiModalPredictor\n",
      "    predictor = MultiModalPredictor.load(\"c:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\autogluon\\AutogluonModels\\ag-20251116_014835\")\n",
      "    ```\n",
      "\n",
      "If you are not satisfied with the model, try to increase the training time, \n",
      "adjust the hyperparameters (https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/customization.html),\n",
      "or post issues on GitHub (https://github.com/autogluon/autogluon/issues).\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "\n",
    "# ---------------------------\n",
    "# Config: CPU Testing with CORAL\n",
    "# ---------------------------\n",
    "TRAIN_CSV = r\"C:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\train_data.csv\"\n",
    "TEST_CSV  = r\"C:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\test_data.csv\"\n",
    "IMG_DIR   = r\"C:\\Users\\z5489720\\OneDrive - UNSW\\TERM 1\\Github\\aglone\\images\"\n",
    "LABEL_COL = \"rating\"\n",
    "IMAGE_COL = \"image_path\"\n",
    "\n",
    "# CPU-only testing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def to_abs(path: str, base: str) -> str:\n",
    "    p = str(path).strip()\n",
    "    return p if os.path.isabs(p) else os.path.abspath(os.path.join(base, p))\n",
    "\n",
    "def add_abs_image_paths(df: pd.DataFrame, image_col: str, base_dir: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[image_col] = df[image_col].astype(str).map(lambda p: to_abs(p, base_dir))\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# Main Test Script\n",
    "# ---------------------------\n",
    "np.random.seed(123)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV, index_col=0)\n",
    "test_df  = pd.read_csv(TEST_CSV,  index_col=0)\n",
    "\n",
    "# Use small subset for quick CPU testing\n",
    "SAMPLE_SIZE = 50  # Reduce to 50 samples for fast testing\n",
    "print(f\"Using {SAMPLE_SIZE} training samples for quick CPU test\")\n",
    "train_df = train_df.sample(n=min(SAMPLE_SIZE, len(train_df)), random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(n=min(20, len(test_df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Absolute image paths\n",
    "train_df = add_abs_image_paths(train_df, IMAGE_COL, IMG_DIR)\n",
    "test_df  = add_abs_image_paths(test_df,  IMAGE_COL, IMG_DIR)\n",
    "\n",
    "# Sanity check: verify files exist\n",
    "missing_train = (~train_df[IMAGE_COL].map(os.path.exists)).sum()\n",
    "missing_test = (~test_df[IMAGE_COL].map(os.path.exists)).sum()\n",
    "print(f\"Missing files - Train: {missing_train}, Test: {missing_test}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(f\"\\nLabel distribution in training data:\")\n",
    "print(train_df[LABEL_COL].value_counts().sort_index())\n",
    "print(f\"\\nUnique classes: {sorted(train_df[LABEL_COL].unique())}\")\n",
    "\n",
    "# CORAL Loss Configuration for CPU Testing\n",
    "# CRITICAL: The config key is \"optim.loss_func\" not \"optim.loss_function\"!\n",
    "coral_config = {\n",
    "    # Loss function - CORAL (use loss_func not loss_function!)\n",
    "    \"optim.loss_func\": \"coral\",  # ‚ö†Ô∏è CORRECT KEY!\n",
    "    \n",
    "    # Minimal training for quick test\n",
    "    \"optim.max_epochs\": 3,\n",
    "    \"optim.learning_rate\": 1e-4,\n",
    "    \n",
    "    # CPU configuration\n",
    "    \"env.num_gpus\": 0,  # Force CPU\n",
    "    \"env.per_gpu_batch_size\": 4,  # Small batch size for CPU\n",
    "    \"env.num_workers\": 0,  # 0 workers on CPU to avoid multiprocessing issues\n",
    "    \"env.num_workers_inference\": 0,\n",
    "    \n",
    "    # Use smaller model for faster CPU testing\n",
    "    \"model.hf_text.checkpoint_name\": \"prajjwal1/bert-tiny\",  # Tiny BERT model\n",
    "    \"model.timm_image.checkpoint_name\": \"resnet18\",  # Small ResNet\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING CORAL LOSS ON CPU\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss function: {coral_config['optim.loss_func']}\")\n",
    "print(f\"Epochs: {coral_config['optim.max_epochs']}\")\n",
    "print(f\"Batch size: {coral_config['env.per_gpu_batch_size']}\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Create predictor\n",
    "predictor = MultiModalPredictor(\n",
    "    label=LABEL_COL,\n",
    "    problem_type=\"multiclass\",  # Treat as multiclass (ordinal)\n",
    "    eval_metric=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Fit with CORAL loss\n",
    "print(\"Starting training with CORAL loss...\")\n",
    "print(\"‚ö†Ô∏è LOOK FOR DEBUG MESSAGES ABOUT LOSS FUNCTION SELECTION ‚ö†Ô∏è\\n\")\n",
    "predictor.fit(\n",
    "    train_data=train_df,\n",
    "    hyperparameters=coral_config,\n",
    "    time_limit=600,  # 10 minute timeout for safety\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f3d9e",
   "metadata": {},
   "source": [
    "## Test Predictions with CORAL\n",
    "\n",
    "Now let's verify that predictions work correctly with CORAL loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231eca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "print(\"\\nTesting predictions on test data...\")\n",
    "predictions = predictor.predict(test_df.drop(columns=[LABEL_COL]))\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"Predictions:\\n{predictions.head(10)}\")\n",
    "print(f\"\\nUnique predicted values: {sorted(predictions.unique())}\")\n",
    "\n",
    "# Test predict_proba\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing predict_proba (probability outputs)...\")\n",
    "probabilities = predictor.predict_proba(test_df.drop(columns=[LABEL_COL]))\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"\\nFirst 5 probability distributions:\")\n",
    "print(probabilities.head())\n",
    "print(f\"\\nProbabilities sum to 1? {np.allclose(probabilities.sum(axis=1), 1.0)}\")\n",
    "\n",
    "# Compare predictions with ground truth\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparing predictions with ground truth...\")\n",
    "comparison = pd.DataFrame({\n",
    "    'True': test_df[LABEL_COL].values,\n",
    "    'Predicted': predictions.values\n",
    "})\n",
    "print(comparison.head(10))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (comparison['True'] == comparison['Predicted']).mean()\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Calculate mean absolute error (useful for ordinal data)\n",
    "mae = np.abs(comparison['True'] - comparison['Predicted']).mean()\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORAL LOSS TEST COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cfd22",
   "metadata": {},
   "source": [
    "## Compare CORAL vs Standard Cross-Entropy (Optional)\n",
    "\n",
    "If you want to compare CORAL with standard cross-entropy loss, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e02f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train with standard CrossEntropyLoss for comparison\n",
    "baseline_config = {\n",
    "    # No CORAL - standard cross-entropy (default)\n",
    "    # \"optim.loss_function\": \"cross_entropy\",  # Default, no need to specify\n",
    "    \n",
    "    \"optim.max_epochs\": 3,\n",
    "    \"optim.learning_rate\": 1e-4,\n",
    "    \n",
    "    \"env.num_gpus\": 0,\n",
    "    \"env.per_gpu_batch_size\": 4,\n",
    "    \"env.num_workers\": 0,\n",
    "    \"env.num_workers_inference\": 0,\n",
    "    \n",
    "    \"model.hf_text.checkpoint_name\": \"prajjwal1/bert-tiny\",\n",
    "    \"model.timm_image.checkpoint_name\": \"resnet18\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING WITH STANDARD CROSS-ENTROPY (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictor_baseline = MultiModalPredictor(\n",
    "    label=LABEL_COL,\n",
    "    problem_type=\"multiclass\",\n",
    "    eval_metric=\"accuracy\",\n",
    "    path=\"./baseline_model\"  # Different path to avoid conflict\n",
    ")\n",
    "\n",
    "predictor_baseline.fit(\n",
    "    train_data=train_df,\n",
    "    hyperparameters=baseline_config,\n",
    "    time_limit=600,\n",
    ")\n",
    "\n",
    "# Compare predictions\n",
    "baseline_preds = predictor_baseline.predict(test_df.drop(columns=[LABEL_COL]))\n",
    "baseline_probs = predictor_baseline.predict_proba(test_df.drop(columns=[LABEL_COL]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: CORAL vs BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True': test_df[LABEL_COL].values,\n",
    "    'CORAL': predictions.values,\n",
    "    'Baseline': baseline_preds.values\n",
    "})\n",
    "print(comparison_df.head(10))\n",
    "\n",
    "coral_acc = (comparison_df['True'] == comparison_df['CORAL']).mean()\n",
    "baseline_acc = (comparison_df['True'] == comparison_df['Baseline']).mean()\n",
    "\n",
    "coral_mae = np.abs(comparison_df['True'] - comparison_df['CORAL']).mean()\n",
    "baseline_mae = np.abs(comparison_df['True'] - comparison_df['Baseline']).mean()\n",
    "\n",
    "print(f\"\\nCORAL Accuracy: {coral_acc:.2%}\")\n",
    "print(f\"Baseline Accuracy: {baseline_acc:.2%}\")\n",
    "print(f\"\\nCORAL MAE: {coral_mae:.3f}\")\n",
    "print(f\"Baseline MAE: {baseline_mae:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
